{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb8eb2f",
   "metadata": {},
   "source": [
    "# Delete the duplicates SMILES in the original FooDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629edb0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# library\n",
    "import pandas as pd\n",
    "\n",
    "# path\n",
    "path = \"/path\"\n",
    "\n",
    "# import csv file\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# xóa duplicate\n",
    "df=data.drop_duplicates(subset=[\"food_smiles\"], keep=\"first\")\n",
    "\n",
    "# Xuất file duplicate\n",
    "df.to_csv(\"/path\",index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd92646",
   "metadata": {},
   "source": [
    "# drug_food_interactions (tanimoto_coefficient) (my-rdkit-env py2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbf2cb",
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "import pandas as pd\n",
    "path = \"/path\"\n",
    "df = pd.read_csv(path)\n",
    "# get food_smiles column\n",
    "df_smiles = df['food_smiles']\n",
    "# a=df.loc[df.food_smiles == ds, 'food_id']\n",
    "# make a list of mols\n",
    "ms = [Chem.MolFromSmiles(x) for x in df_smiles]\n",
    "\n",
    "# make a list of fingerprints (fp)\n",
    "fps = [FingerprintMols.FingerprintMol(x) for x in ms]\n",
    "\n",
    "# the list for the dataframe\n",
    "id_query, query, id_target, target, similarity = [], [], [], [], []\n",
    "\n",
    "# compare all fp pairwise without duplicates\n",
    "for n in range(len(fps)-1): # -1 so the last fp will not be used\n",
    "    s = DataStructs.BulkTanimotoSimilarity(fps[n], fps[n+1:]) # +1 compare with the next to the last fp\n",
    "#     print(df_smiles[n], df_smiles[n+1:]) # witch mol is compared with what group\n",
    "    # collect the SMILES and values\n",
    "    for m in range(len(s)):\n",
    "        if s[m]>=0.75 and list(df.loc[df.food_smiles == df_smiles.tolist()[n+1:][m],'food_id']) not in id_target:\n",
    "#             query.append(df_smiles[n])\n",
    "#             id_query.append(list(df.loc[df.food_smiles == df_smiles[n], 'food_id']))\n",
    "#             target.append(df_smiles.tolist()[n+1:][m])\n",
    "            id_target.append(list(df.loc[df.food_smiles == df_smiles.tolist()[n+1:][m],'food_id']))\n",
    "            similarity.append(s[m])\n",
    "# Print similarity\n",
    "d = {\"id_target\":id_target,'Similarity': similarity}\n",
    "df_final = pd.DataFrame(data=d)\n",
    "df_final = df_final.sort_values('Similarity', ascending=False)\n",
    "# df_final.to_csv('file_tani_2.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca172f8",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Delete all food compounds with Tanimoto coefficient more than 0.75 from the non-duplicates data set\n",
    "path_ori = \"/path\"\n",
    "df_original=pd.read_csv(path_ori)\n",
    "df_original.shape\n",
    "new_df=df_original[~df_original.food_id.isin(mid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54823fe",
   "metadata": {},
   "source": [
    "# getting 3,780 features  from every pairs of drug-food constituents (my-rdkit-env py2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabb21a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "from PyBioMed.PyInteraction import PyInteraction \n",
    "from PyBioMed.PyMolecule import moe \n",
    "from rdkit import Chem   \n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "data=pd.read_csv('C:/Users/Henry Kha/Google Drive/Nghien cuu/Drug food Project/database/data for feature extraction - Copy (3).csv')\n",
    "\n",
    "drug_smiles = data['drug_smiles']\n",
    "food_smiles = data['food_smiles']\n",
    "\n",
    "# check drug\n",
    "for i in range (0,n):\n",
    "    m_drug = Chem.MolFromSmiles(drug_smiles[i])\n",
    "    mol_des_drug=moe.GetMOE(m_drug)\n",
    "# check food\n",
    "for j in range (0,n):\n",
    "    m_food = Chem.MolFromSmiles(food_smiles[j])\n",
    "    mol_des_food=moe.GetMOE(m_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9beba",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Obtain all\n",
    "for i in range (0,n):\n",
    "#     print(i)\n",
    "    m_drug = Chem.MolFromSmiles(drug_smiles[i])\n",
    "    mol_des_drug = moe.GetMOE(m_drug)\n",
    "    for j in range (0,n): #(#0:501, 501:1001, 1001:1501, 1501:2001, 2001:2501, 2501:2581)\n",
    "#         print(j)\n",
    "        m_food = Chem.MolFromSmiles(food_smiles[j]) \n",
    "        mol_des_food=moe.GetMOE(m_food) \n",
    "        mol_mol_interaction1 = PyInteraction.CalculateInteraction1(mol_des_drug,mol_des_food) \n",
    "        mol_mol_interaction2 = PyInteraction.CalculateInteraction2(mol_des_drug,mol_des_food)\n",
    "        mol_mol_interaction3 = PyInteraction.CalculateInteraction3(mol_des_drug,mol_des_food) \n",
    "        mol_mol_interaction1.update(mol_mol_interaction2)\n",
    "        mol_mol_interaction1.update(mol_mol_interaction3)\n",
    "        # save csv file\n",
    "        if os.path.isfile(\"/path\"):\n",
    "            with open('/path', \n",
    "                  'ab') as f:\n",
    "                w = csv.DictWriter(f, mol_mol_interaction1.keys())\n",
    "                w.writerow(mol_mol_interaction1)\n",
    "        else:\n",
    "            with open('/path', \n",
    "                  'wb') as f:\n",
    "                w = csv.DictWriter(f, mol_mol_interaction1.keys())\n",
    "                w.writeheader()\n",
    "                w.writerow(mol_mol_interaction1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0aa2a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Too large data base? --> Randomization\n",
    "for j in range (0,n):\n",
    "    m_food = Chem.MolFromSmiles(food_smiles[j]) \n",
    "    mol_des_food=moe.GetMOE(m_food)\n",
    "    \n",
    "    a = random.randint(0,n)\n",
    "    b = random.randint(0,n)\n",
    "    c = random.randint(0,n)\n",
    "    \n",
    "    m_drug_a = Chem.MolFromSmiles(drug_smiles[a])\n",
    "    mol_des_drug_a = moe.GetMOE(m_drug_a)\n",
    "    \n",
    "    m_drug_b = Chem.MolFromSmiles(drug_smiles[b])\n",
    "    mol_des_drug_b = moe.GetMOE(m_drug_b)\n",
    "    \n",
    "    m_drug_c = Chem.MolFromSmiles(drug_smiles[c])\n",
    "    mol_des_drug_c = moe.GetMOE(m_drug_c)\n",
    "    \n",
    "    mol_mol_interaction1_a = PyInteraction.CalculateInteraction1(mol_des_drug_a,mol_des_food) \n",
    "    mol_mol_interaction2_a = PyInteraction.CalculateInteraction2(mol_des_drug_a,mol_des_food)\n",
    "    mol_mol_interaction3_a = PyInteraction.CalculateInteraction3(mol_des_drug_a,mol_des_food) \n",
    "    mol_mol_interaction1_a.update(mol_mol_interaction2_a)\n",
    "    mol_mol_interaction1_a.update(mol_mol_interaction3_a)\n",
    "    \n",
    "    mol_mol_interaction1_b = PyInteraction.CalculateInteraction1(mol_des_drug_b,mol_des_food) \n",
    "    mol_mol_interaction2_b = PyInteraction.CalculateInteraction2(mol_des_drug_b,mol_des_food)\n",
    "    mol_mol_interaction3_b = PyInteraction.CalculateInteraction3(mol_des_drug_b,mol_des_food) \n",
    "    mol_mol_interaction1_b.update(mol_mol_interaction2_b)\n",
    "    mol_mol_interaction1_b.update(mol_mol_interaction3_b)\n",
    "    \n",
    "    mol_mol_interaction1_c = PyInteraction.CalculateInteraction1(mol_des_drug_c,mol_des_food) \n",
    "    mol_mol_interaction2_c = PyInteraction.CalculateInteraction2(mol_des_drug_c,mol_des_food)\n",
    "    mol_mol_interaction3_c = PyInteraction.CalculateInteraction3(mol_des_drug_c,mol_des_food) \n",
    "    mol_mol_interaction1_c.update(mol_mol_interaction2_c)\n",
    "    mol_mol_interaction1_c.update(mol_mol_interaction3_c)\n",
    "    \n",
    "    # save csv file\n",
    "    if os.path.isfile(\"C:/Users/Henry Kha/Google Drive/Nghien cuu/Drug food Project/database/label 1/drug vs food (1).csv\"):\n",
    "        with open('C:/Users/Henry Kha/Google Drive/Nghien cuu/Drug food Project/database/label 1/drug vs food (1).csv', \n",
    "                  'ab') as f:\n",
    "            w = csv.DictWriter(f, mol_mol_interaction1_a.keys())\n",
    "            w.writerow(mol_mol_interaction1_a)\n",
    "            \n",
    "            w = csv.DictWriter(f, mol_mol_interaction1_b.keys())\n",
    "            w.writerow(mol_mol_interaction1_b)\n",
    "            \n",
    "            w = csv.DictWriter(f, mol_mol_interaction1_c.keys())\n",
    "            w.writerow(mol_mol_interaction1_c)\n",
    "    else:\n",
    "        with open('C:/Users/Henry Kha/Google Drive/Nghien cuu/Drug food Project/database/label 1/drug vs food (1).csv', \n",
    "                  'wb') as f:\n",
    "            w = csv.DictWriter(f, mol_mol_interaction1_a.keys())\n",
    "            w.writeheader()\n",
    "            w.writerow(mol_mol_interaction1_a)\n",
    "            w = csv.DictWriter(f, mol_mol_interaction1_b.keys())\n",
    "            w.writerow(mol_mol_interaction1_b)\n",
    "            w = csv.DictWriter(f, mol_mol_interaction1_c.keys())\n",
    "            w.writerow(mol_mol_interaction1_c)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef6d6f",
   "metadata": {},
   "source": [
    "# Labelling data into three classes and creating five sample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fae254",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# make column label\n",
    "make_col_label=pd.DataFrame(index=np.arange(number-of-samples), columns=np.arange(1))\n",
    "# b.fillna(0)\n",
    "make_col_label=make_col_label.replace(np.nan, num-class)\n",
    "col_one_list = make_col_label[0].tolist()\n",
    "df_0.insert(loc=0, column='LABEL', value=col_one_list)\n",
    "# the same with other labels\n",
    "\n",
    "### getting only 10,000 samples randomly from each label to append into one file ###\n",
    "df_0 = df_0.sample(n=10000)\n",
    "df_merged=df_0.append(df_1).append(df_2) # The training data set contains 30,000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eeef86",
   "metadata": {},
   "source": [
    "# Use LazyPredict Classifier to see which algorithms work best on five sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ec301",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "df=pd.read_csv(\"C:/Users/Henry Kha/Google Drive/Nghien cuu/Drug food Project/database/label_2/2bbb.csv\")\n",
    "X = df.drop('LABEL', axis=1)\n",
    "y = df[\"LABEL\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "# save five results\n",
    "predictions.to_csv(\"/path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ec677",
   "metadata": {},
   "source": [
    "# Feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing features with low variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.8) # càng tăng threshold càng loại nhiều features\n",
    "# Features with a training-set variance lower than this threshold will\n",
    "# be removed. The default is to keep all features with non-zero variance,\n",
    "# i.e. remove the features that have the same value in all samples.\n",
    "X=sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "i_value, lightgbm, kbest_xgb, dtc, rf = [], [], [], [], []\n",
    "for i in range(1,2849):\n",
    "    X = df.drop('LABEL', axis=1)\n",
    "    X = SelectKBest(score_func=f_classif, k=i).fit_transform(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        random_state=1000)\n",
    "\n",
    "    clf_xgb.fit(X_train, y_train).predict(X_test)\n",
    "    score_xgb=clf_xgb.score(X_test,y_test)\n",
    "\n",
    "    i_value.append(i)\n",
    "    kbest_xgb.append(score_xgb)\n",
    "    \n",
    "\n",
    "d = {\"i_value\":i_value,'xgb':kbest_xgb}\n",
    "df_final = pd.DataFrame(data=d)\n",
    "df_final.to_csv(\"...\",\n",
    "                index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c72396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SelectKBest with K = 2844\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "X = SelectKBest(score_func=f_classif, k=2844).fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc42ed",
   "metadata": {},
   "source": [
    "# Applying four algorithms on one training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1000)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "# library\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf_lgbm = lightgbm.LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    subsample_for_bin=200000,\n",
    "    objective=None,\n",
    "    class_weight=None,\n",
    "    min_split_gain=0.0,\n",
    "    min_child_weight=0.001,\n",
    "    min_child_samples=20,\n",
    "    subsample=1.0,\n",
    "    subsample_freq=0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    silent=True,\n",
    "    importance_type='split',\n",
    ")\n",
    "clf_lgbm=clf_lgbm.fit(X_train,y_train)\n",
    "y_pred_lgbm=clf_lgbm.predict(X_test)\n",
    "#importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred_lgbm)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "clf_xgb = xgb.XGBClassifier()\n",
    "# fit and train and predict\n",
    "clf_xgb=clf_xgb.fit(X_train,y_train)\n",
    "y_pred_xgb=clf_xgb.predict(X_test)\n",
    "#importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred_xgb)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# random forest\n",
    "clf_rf=RandomForestClassifier(n_estimators=100,\n",
    "                       criterion='gini',\n",
    "                       max_depth=None,\n",
    "                       min_samples_split=2,\n",
    "                       min_samples_leaf=1,\n",
    "                       min_weight_fraction_leaf=0.0,\n",
    "                       max_features='auto',\n",
    "                       max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None,\n",
    "                       bootstrap=True,\n",
    "                       oob_score=False,\n",
    "                       n_jobs=None,\n",
    "                       random_state=None,\n",
    "                       verbose=0,\n",
    "                       warm_start=False,\n",
    "                       class_weight=None,\n",
    "                       ccp_alpha=0.0,\n",
    "                       max_samples=None)\n",
    "# fit and train and predict\n",
    "clf_rf=clf_rf.fit(X_train,y_train)\n",
    "y_pred_rf=clf_rf.predict(X_test)\n",
    "#importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred_rf)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8828eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# random forest\n",
    "clf_etc = ExtraTreesClassifier(n_estimators=100,\n",
    "                               criterion='gini',\n",
    "                               max_depth=None,\n",
    "                               min_samples_split=2,\n",
    "                               min_samples_leaf=1,\n",
    "                               min_weight_fraction_leaf=0.0,\n",
    "                               max_features='auto',\n",
    "                               max_leaf_nodes=None,\n",
    "                               min_impurity_decrease=0.0,\n",
    "                               min_impurity_split=None,\n",
    "                               bootstrap=False,\n",
    "                               oob_score=False,\n",
    "                               n_jobs=None,\n",
    "                               random_state=None,\n",
    "                               verbose=0,\n",
    "                               warm_start=False,\n",
    "                               class_weight=None,\n",
    "                               ccp_alpha=0.0,\n",
    "                               max_samples=None)\n",
    "# fit and train and predict\n",
    "clf_etc=clf_etc.fit(X_train,y_train)\n",
    "y_pred_etc=clf_etc.predict(X_test)\n",
    "#importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred_etc)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a3f49",
   "metadata": {},
   "source": [
    "# SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "f=plt.figure()\n",
    "shap_values = shap.TreeExplainer(clf_xgb).shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "# for concrete labels\n",
    "f=plt.figure()\n",
    "shap.summary_plot(shap_values[2], X_train)\n",
    "f.savefig(\"importance_label_2.png\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07627bb",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2aa19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = '...'\n",
    "pickle.dump(clf_xgb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "import pickle\n",
    "loaded_model = pickle.load(open(\"...\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45df21",
   "metadata": {},
   "source": [
    "# Drug-food recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf993af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction= pd.read_csv(\"...\")\n",
    "# new_output = clf_lgbm.predict(prediction)\n",
    "feature_name=pd.read_csv(\n",
    "    \"...\"\n",
    ")\n",
    "feature_name=list(feature_name.columns.values)\n",
    "feature_name.pop(0)\n",
    "len(feature_name)\n",
    "prediction = prediction.loc[:,feature_name]\n",
    "new_output = clf_xgb.predict(prediction)\n",
    "\n",
    "if new_output==0:\n",
    "    print(\"A should not be taken with food containing B\")\n",
    "elif new_output==1:\n",
    "    print('A could be taken with food containing B')\n",
    "else:\n",
    "    print(\"A may be taken with food containing B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
